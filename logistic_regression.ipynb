{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ques 5: Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5a. Implementation of logistic regression classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, learning_rate, max_iter):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights, self.bias, self.losses = 0, 0, []\n",
    "\n",
    "    def sigmoid(self, logits):\n",
    "        return 1/(1+np.exp(-1*logits))\n",
    "   \n",
    "    def loss(self, targets, predictions):\n",
    "        loss = -1*np.mean(targets*(np.log(predictions)) + (1-targets)*(np.log(1-predictions)))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Computation of gradients.\n",
    "    def backward(self, inputs, targets, predictions):\n",
    "        num_samples = inputs.shape[0]\n",
    "        dw = (1/num_samples) * (np.dot(inputs.T, (predictions - targets)))\n",
    "        db = (1/num_samples) * (np.sum((predictions - targets)))\n",
    "        \n",
    "        return dw/num_samples, db/num_samples\n",
    "#         return dw, db\n",
    "    \n",
    "    # Fit function for training the model.\n",
    "    def fit(self, inputs, targets):\n",
    "        losses = []\n",
    "        num_samples, feature_dim = inputs.shape\n",
    "#         wt_matrix, bias = np.zeros((feature_dim, 1)), 0\n",
    "        # Using the same weights as given in the next question for simplicity.\n",
    "        wt_matrix, bias = np.array([1.5, 0.5]).reshape((feature_dim, 1)), -1\n",
    "        targets = targets.reshape(num_samples, 1)\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            # Forward Pass\n",
    "            logits = np.dot(inputs, wt_matrix) + bias\n",
    "#             print('Logits: ', logits)\n",
    "            predictions = self.sigmoid(logits)\n",
    "            predictions = predictions.reshape(num_samples, 1)\n",
    "            loss = self.loss(targets, predictions)\n",
    "\n",
    "            # Backward Pass\n",
    "            dw, db = self.backward(inputs, targets, predictions)\n",
    "#             print('Gradients: ', dw, db)\n",
    "\n",
    "            # Parameters Update step\n",
    "            wt_matrix -= self.learning_rate*dw\n",
    "            bias -= self.learning_rate*db\n",
    "            losses.append(loss)\n",
    "#             print('Updated wts: ', wt_matrix)\n",
    "#             print('Updated bias: ', bias)\n",
    "\n",
    "            if not epoch%4:\n",
    "                print('Epoch {}:\\tloss --> {:.4f}'.format(epoch+1, loss))\n",
    "                \n",
    "        self.weights, self.bias, self.losses = wt_matrix, bias, losses\n",
    "        \n",
    "        return \n",
    "    \n",
    "    # Predict utility for making predictions.\n",
    "    def predict(self, inputs):\n",
    "        logits = np.dot(inputs, self.weights) + self.bias\n",
    "        outputs = self.sigmoid(logits)\n",
    "        predictions = [1 if output>=0.5 else 0 for output in outputs]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    # Score utility for metric (accuracy) computation.\n",
    "    def score(self, inputs, targets):\n",
    "        predictions = self.predict(inputs)\n",
    "#         print('Predictions: ', predictions)\n",
    "        accuracy = np.sum(targets == predictions) / targets.shape[0]\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this piece of code to udpate parameters for each sample per epoch. The results were getting saturated at an accuracy of 0.833 here also, so opted for other approach.\n",
    "\n",
    "# class LogisticRegression_per_sample_update():\n",
    "#     def __init__(self, learning_rate, max_iter):\n",
    "#         super(LogisticRegression_per_sample_update, self).__init__()\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.max_iter = max_iter\n",
    "#         self.weights, self.bias, self.losses = 0, 0, []\n",
    "#         self.ss = StandardScaler()\n",
    "\n",
    "#     def sigmoid(self, logit):\n",
    "#         return 1/(1+np.exp(-1*logit))\n",
    "   \n",
    "#     def loss(self, target, prediction):\n",
    "#         loss = -1 * target*(np.log(prediction)) + (1-target)*(np.log(1-prediction))\n",
    "        \n",
    "#         return loss\n",
    "    \n",
    "#     def backward(self, input, target, prediction):\n",
    "#         input = input.reshape(input.shape[0], 1)\n",
    "#         dw = np.dot(input, (prediction - target).reshape(1,1))\n",
    "#         db = np.sum((prediction - target))\n",
    "        \n",
    "#         return dw, db\n",
    "    \n",
    "#     def fit(self, inputs, targets):\n",
    "#         losses = []\n",
    "#         num_samples, feature_dim = inputs.shape\n",
    "# #         wt_matrix, bias = np.zeros((feature_dim, 1)), 0\n",
    "#         wt_matrix, bias = np.array([1.5, 0.5]).reshape((feature_dim, 1)), -1\n",
    "#         targets = targets.reshape(num_samples, 1)\n",
    "#         inputs = self.ss.fit_transform(inputs)\n",
    "        \n",
    "#         for epoch in range(self.max_iter):\n",
    "#             loss_per_epoch=0\n",
    "#             for i in range(num_samples):\n",
    "#             #Forward Pass\n",
    "#                 logit = np.dot(inputs[i], wt_matrix)+bias\n",
    "#                 prediction = self.sigmoid(logit)\n",
    "# #                 prediction = prediction.reshape(num_samples, 1)\n",
    "#                 loss_per_sample = self.loss(targets[i], prediction)\n",
    "\n",
    "#                 #Backward Pass\n",
    "#                 dw, db = self.backward(inputs[i], targets[i], prediction)\n",
    "\n",
    "#                 #Parameters Update step\n",
    "#                 wt_matrix -= self.learning_rate*dw\n",
    "#                 bias -= self.learning_rate*db\n",
    "#                 loss_per_epoch += loss_per_sample\n",
    "                \n",
    "#             loss_per_epoch /= num_samples\n",
    "#             losses.append(loss_per_epoch[0])\n",
    "#             if not epoch%9:\n",
    "#                 print('Epoch {}:\\tloss --> {:.4f}'.format(epoch+1, loss_per_epoch[0]))\n",
    "                \n",
    "#         self.weights, self.bias, self.losses = wt_matrix, bias, losses\n",
    "        \n",
    "#         return \n",
    "    \n",
    "#     def predict(self, inputs):\n",
    "#         inputs = self.ss.fit_transform(inputs)\n",
    "#         logits = np.dot(inputs, self.weights) + self.bias\n",
    "#         outputs = self.sigmoid(logits)\n",
    "#         predictions = [1.0 if output>=0.5 else 0.0 for output in outputs]\n",
    "#         return np.array(predictions)\n",
    "    \n",
    "#     def score(self, inputs, targets):\n",
    "#         predictions = self.predict(inputs)\n",
    "#         accuracy = np.sum(targets == predictions) / targets.shape[0]\n",
    "        \n",
    "#         return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating training and test datasets as given and splitting it into features and targets for training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([[0.346, 0.780, 0.0],[0.303, 0.439, 0.0], [0.358, 0.729, 0.0], [0.602, 0.863, 1.0], [0.790, 0.753, 1.0], [0.611, 0.965, 1.0]])\n",
    "test = np.array([[0.959, 0.382, 0.0], [0.750, 0.306, 0.0], [0.395, 0.760, 0.0], [0.823, 0.764, 1.0], [0.761, 0.874, 1.0], [0.844, 0.435, 1.0]])\n",
    "\n",
    "train = train.reshape((6,3))\n",
    "test = test.reshape((6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.346, 0.78 , 0.   ],\n",
       "       [0.303, 0.439, 0.   ],\n",
       "       [0.358, 0.729, 0.   ],\n",
       "       [0.602, 0.863, 1.   ],\n",
       "       [0.79 , 0.753, 1.   ],\n",
       "       [0.611, 0.965, 1.   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.358, 0.729, 0.   ],\n",
       "       [0.611, 0.965, 1.   ],\n",
       "       [0.602, 0.863, 1.   ],\n",
       "       [0.303, 0.439, 0.   ],\n",
       "       [0.79 , 0.753, 1.   ],\n",
       "       [0.346, 0.78 , 0.   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.959, 0.382, 0.   ],\n",
       "       [0.75 , 0.306, 0.   ],\n",
       "       [0.395, 0.76 , 0.   ],\n",
       "       [0.823, 0.764, 1.   ],\n",
       "       [0.761, 0.874, 1.   ],\n",
       "       [0.844, 0.435, 1.   ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train[:, 0:2], train[:, 2]\n",
    "X_test, y_test = test[:, 0:2], test[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5b: Running the model for given data.** \n",
    "<font size=\"4\">**(i) Logistic Model P(y_hat=1 | x1, x2)** = sigmoid( theta<sub>0</sub> + theta<sub>1</sub>.x<sub>1</sub> + theta<sub>2</sub>.x<sub>2</sub>) => **sigmoid( -1 + 1.5\\*x<sub>1</sub> + 0.5\\*x<sub>2</sub> )**</br>\n",
    "**Cross Entropy Error (y=y, y_hat=1)** = -(y\\*log(1) + (1-y)\\*log(1-1)) => **-1\\* ( 0 + (1-y) \\* log(0) )**</font>\n",
    "\n",
    "<font size=\"4\">**(ii):** Gradient descent to update parameters for 1 iteration (*_Comment out zero initialization and uncomment given initialization weights and bias_*).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss --> 0.5570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.50089181],\n",
       "        [0.50032811]]), -1.000527709962094)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(learning_rate=0.1, max_iter=1)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.weights, lr.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> **Updated logistic regression model:** 1 / ( 1 + e<sup>-( -1.0005 + 1.5008\\*x<sub>1</sub> + 0.5003\\*x<sub>2</sub> )</sup> ) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss --> 0.5570\n",
      "Epoch 5:\tloss --> 0.5567\n",
      "Epoch 9:\tloss --> 0.5564\n",
      "Accuracy: 0.6667\n",
      "Precision: 0.6000\n",
      "Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(learning_rate=0.1, max_iter=10)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "acc = lr.score(X_test, y_test)\n",
    "precision, recall = precision_score(y_test, y_pred), recall_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}\\nPrecision: {:.4f}\\nRecall: {:.4f}'.format(acc, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
