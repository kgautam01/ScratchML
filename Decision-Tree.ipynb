{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dda11d7",
   "metadata": {},
   "source": [
    "# <span style='color:Blue'>Question 3</span>\n",
    "### <span style='color:Blue'>1. Decision Tree Implementation (impurity measure: entropy)</span>\n",
    "### <span style='color:Blue'>2. K-Fold Cross Validation (K=10)</span>\n",
    "### <span style='color:Blue'>3. Improvement Strategies (Gini-Index and Post-Pruning)</span>\n",
    "***\n",
    "**NOTE:**\n",
    "<p style='text-align: justify;'>\n",
    "1. Below mentioned code is the combined implementation of Decision Tree for a classification problem using entropy and gini as choices for the impurity measure in the tree. You just need to pass appropriate name of the impurity_measure to be used in the tree.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "2. Comments have been added in every module to explain the concept used in the implementation for the very module.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "3. Decision Tree Representation is done using a nested dictionary where each key value is a node and values are the left (0th index) and right child (1st index) which can be further nested in case of internal nodes/ decision nodes.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "4. To keep the implementations combined, rather than finding the min-gini-index, max is found out after negating the weighted gini-index so that same logic for both entropy and gini-index can be used in the splitting of the node.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "5. Random shuffling on the data is done before creating chunks. The whole data is used to create 10 chunks from which one is used for validation and remanining for training. The reports at the bottom contains validation set accuracy (averaged over K runs) for initial implementation (using entropy) and improved implementation (using gini-index).</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "6. Two result files are generated containing averaged validation accuracies for both implementations.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "7. Filenames for the result files have been changed to distinguish between the files. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f579342",
   "metadata": {},
   "source": [
    "## _Implementation using Python_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20749c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FoML Assign 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Kuldeep Gautam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9653d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your decision tree below\n",
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self, attr_names, impurity_measure):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        self.tree = {}\n",
    "        # List of attribute names, used for creating nodes in the decision tree as well as in prediction.\n",
    "        self.attr_names = attr_names\n",
    "        # Type of impurity measure to be used in building the decision tree.\n",
    "        self.impurity_measure = impurity_measure\n",
    "    \n",
    "    \n",
    "    # Computing the potential threshold values for every column for creating best splits.       \n",
    "    def compute_threshold_values(self, data, threshold_values):\n",
    "        target = data[:, -1]\n",
    "        for column_idx in range(data.shape[1]-1):\n",
    "            if not threshold_values[column_idx]==False:\n",
    "                temp_thr_list = []\n",
    "                for idx in range(len(target)-1):\n",
    "                    # Only considering those indexes whereever there is a class change in the target column.\n",
    "                    if target[idx] != target[idx+1]:\n",
    "                        thr = (data[idx, column_idx] + data[idx, column_idx])/2\n",
    "                        temp_thr_list.append(thr)\n",
    "\n",
    "                threshold_values[column_idx] = list(set(temp_thr_list))\n",
    "        return threshold_values\n",
    "\n",
    "    \n",
    "    # Computing the entropy measure for a set of data points.\n",
    "    def get_entropy(self, data):\n",
    "        targets = data[:,-1]\n",
    "        classes, classes_count = np.unique(targets, return_counts=True)\n",
    "\n",
    "        # Case of min entropy\n",
    "        if len(classes) == 1:\n",
    "            entropy = float(0)\n",
    "\n",
    "        # Case of max entropy\n",
    "        elif len(classes) == 2 and len(set(classes_count)) == 1:\n",
    "            entropy = float(1)\n",
    "\n",
    "        # Other cases\n",
    "        else:\n",
    "            class_probs = classes_count/sum(classes_count)\n",
    "            entropy = sum(-1 * class_probs * np.log2(class_probs))\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    # Computing Gini-Indedx measure for a set of data points.\n",
    "    def get_gini(self, data):\n",
    "        targets = data[:,-1]\n",
    "        classes, classes_count = np.unique(targets, return_counts=True)\n",
    "\n",
    "        if len(classes) == 1:\n",
    "            gini = float(0)\n",
    "\n",
    "        elif len(classes) == 2 and len(set(classes_count)) == 1:\n",
    "            gini = 0.5\n",
    "\n",
    "        else:\n",
    "            class_probs = classes_count/sum(classes_count)\n",
    "            gini = 1 - sum(np.square(class_probs))\n",
    "\n",
    "        return gini\n",
    "\n",
    "    \n",
    "    # Computing the weighted child entropy or weighted child gini-index values using the probabilities.\n",
    "    def get_weighted_impurity(self, left_dtree, right_dtree):\n",
    "        possible_outcomes = left_dtree.shape[0] + right_dtree.shape[0]\n",
    "        prob_left = left_dtree.shape[0]/possible_outcomes\n",
    "        prob_right = right_dtree.shape[0]/possible_outcomes\n",
    "\n",
    "        if self.impurity_measure == 'entropy':\n",
    "            weighted_impurity = prob_left*self.get_entropy(left_dtree) + prob_right*self.get_entropy(right_dtree)\n",
    "\n",
    "        else:\n",
    "            weighted_impurity = prob_left*self.get_gini(left_dtree) + prob_right*self.get_gini(right_dtree)\n",
    "        \n",
    "        return weighted_impurity\n",
    "\n",
    "    \n",
    "    # Finding the best attribute to create a split. Note that to keep the implementations combined, rather than\n",
    "    # finding the min-weighted gini-index for split max of -(weighted gini-index) is found.\n",
    "    def get_best_attr(self, data, threshold_values):\n",
    "        best_attr_data = {}\n",
    "        best_gain = -1*np.inf\n",
    "        \n",
    "        if self.impurity_measure == 'entropy':\n",
    "            parent_impurity = self.get_entropy(data)\n",
    "            \n",
    "        else:\n",
    "            parent_impurity = float(0)\n",
    "\n",
    "        # Calculation of entropy remains same even if parent column exists in the data. \n",
    "        # Just the number of rows should be filtered after selecting best attribute.\n",
    "        for col_idx, thr_values in threshold_values.items():\n",
    "            if not thr_values==False:\n",
    "                for thr in thr_values:\n",
    "                    # Split data on thr\n",
    "                    left_dtree, right_dtree = data[data[:, col_idx] <= thr], data[data[:, col_idx] > thr]\n",
    "\n",
    "                    # Compute best info gain/gini-index\n",
    "                    if left_dtree.shape[0] > 0 and right_dtree.shape[0] > 0:\n",
    "                        weighted_child_impurity = self.get_weighted_impurity(left_dtree, right_dtree)\n",
    "                        info_gain  =  parent_impurity - weighted_child_impurity\n",
    "\n",
    "                        if info_gain > best_gain:\n",
    "                            best_attr_data['best_attr_idx'] = col_idx\n",
    "                            best_attr_data['best_attr_thr'] = thr\n",
    "                            best_gain = info_gain\n",
    "        \n",
    "        return best_attr_data, best_gain\n",
    "\n",
    "    \n",
    "    # Heart of the algorithm which builds the decision tree based on based splits and choosed impurity measure.\n",
    "    def learn(self, data, threshold_values):\n",
    "        uniq_targets, count_targets = np.unique(data[:, -1], return_counts=True)\n",
    "\n",
    "        # If it is pure node or no more attributes left, return the max class label\n",
    "        if self.impurity_measure=='entropy' and (self.get_entropy(data) == float(0) or not any(threshold_values.values())==True):\n",
    "            return uniq_targets[np.argmax(count_targets)]\n",
    "\n",
    "        # If not a pure node but contains equal number of classes, return 0 as class label (default choice)\n",
    "        elif self.impurity_measure=='entropy' and (self.get_entropy(data) == float(1) or not any(threshold_values.values())==True): \n",
    "            return float(0)\n",
    "\n",
    "        elif self.impurity_measure=='gini' and (self.get_gini(data) == float(0) or not any(threshold_values.values())==True):\n",
    "            return uniq_targets[np.argmax(count_targets)]\n",
    "\n",
    "        # If not a pure node but contains equal number of classes, return 0 as class label (default choice)\n",
    "        elif self.impurity_measure=='gini' and (self.get_gini(data) == 0.5 or not any(threshold_values.values())==True):       \n",
    "            return float(0)\n",
    "\n",
    "        else:\n",
    "            threshold_values = self.compute_threshold_values(data, threshold_values)\n",
    "            best_attr_data, best_gain = self.get_best_attr(data, threshold_values)\n",
    "\n",
    "            # In case of leaf node, best_gain var is not updated and hence to avoid that, this condition is put.\n",
    "            # Taking condition >-1 works for combined implementation of entropy and gini.\n",
    "            # Min value for gini can be -1 (for this implementation) and entropy will be > 0 if attributes are still left)\n",
    "            if best_gain > -1:\n",
    "                best_attr_idx = best_attr_data['best_attr_idx']\n",
    "                best_attr_thr = best_attr_data['best_attr_thr']\n",
    "                node_condition = '{} <= {}'.format(self.attr_names[best_attr_idx], best_attr_thr)\n",
    "                dtree = {node_condition: []}\n",
    "                child_threshold_values = threshold_values.copy()\n",
    "                child_threshold_values[best_attr_idx] = False\n",
    "\n",
    "                left_dtree  = data[data[:, best_attr_idx] <= best_attr_thr]\n",
    "                right_dtree  = data[data[:, best_attr_idx] > best_attr_thr]\n",
    "                dtree[node_condition].append(self.learn(left_dtree, threshold_values=child_threshold_values))\n",
    "                dtree[node_condition].append(self.learn(right_dtree, threshold_values=child_threshold_values))\n",
    "                return dtree\n",
    "        \n",
    "\n",
    "    # Classify/Predict function to make predictions using the learnt decision tree.\n",
    "    def classify(self, test_instance, tree):\n",
    "        node_condition = next(iter(tree.keys()))\n",
    "        attr_name, thr = node_condition.split(' <= ')\n",
    "\n",
    "        if test_instance[self.attr_names.index(attr_name)] <= float(thr):\n",
    "            result = tree[node_condition][0] # left child\n",
    "\n",
    "        else:\n",
    "            result = tree[node_condition][1] # right child\n",
    "\n",
    "        # Checking if the result is itself a dictionary i.e, a decision node. If yes, keep recusring.\n",
    "        if isinstance(result, dict):\n",
    "            return self.classify(test_instance, result)\n",
    "\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "        \n",
    "# Main function building decision tree and performing classification\n",
    "def run_decision_tree(impurity_measure):\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        # next(f, None)\n",
    "        attr_names = f.readline().split(',')[:-1]\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\", quoting=csv.QUOTE_NONNUMERIC)]\n",
    "\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "#     Split into training/test sets\n",
    "#     training_set = [x for i, x in enumerate(data) if i % K != 9]\n",
    "#     test_set = [x for i, x in enumerate(data) if i % K == 9]\n",
    "    \n",
    "    # K-Fold Cross Validation. (K=10)\n",
    "    K = 10\n",
    "    fold_size = len(data)//K\n",
    "    kfold_cv_acc = []\n",
    "    t = []\n",
    "    tree = DecisionTree( attr_names=attr_names, impurity_measure=impurity_measure )\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    for i in range(K):\n",
    "        print('\\nFOLD-{}'.format(i+1) + '='*10)\n",
    "        start = i*fold_size\n",
    "        end = start + fold_size\n",
    "        # valid_set = training_set[start: end]\n",
    "        # train_set = training_set[:start] + training_set[end:]\n",
    "        valid_set = data[start: end]\n",
    "        train_set = data[:start] + data[end:]\n",
    "\n",
    "        threshold_values = {}\n",
    "        for idx in range(len(attr_names)):\n",
    "            threshold_values[idx] = True\n",
    "\n",
    "        # Construct a tree using training set\n",
    "        start = time.time()\n",
    "        tree.tree = tree.learn( np.array(train_set), threshold_values )\n",
    "        end = time.time()-start\n",
    "        t.append(end)\n",
    "\n",
    "        # Validate results on validation set\n",
    "        val_results = []\n",
    "        for instance in valid_set:\n",
    "            result = tree.classify( np.array(instance[:-1]), tree.tree )\n",
    "            val_results.append( result == instance[-1] )\n",
    "\n",
    "        val_accuracy = val_results.count(True)/len(val_results)\n",
    "        print(\"val accuracy: %.4f \" % val_accuracy)\n",
    "        kfold_cv_acc.append(val_accuracy)\n",
    "\n",
    "    accuracy = sum(kfold_cv_acc)/K\n",
    "    print('\\nK-Fold CV Accuracy: {:.4f}'.format(accuracy))\n",
    "    print('Time taken in learning Decision Tree using {}: {:.3f} seconds'.format(impurity_measure, sum(t)/K))\n",
    "    \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"-{}-result.txt\".format(impurity_measure), \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ff811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "\n",
      "FOLD-1==========\n",
      "val accuracy: 0.8016 \n",
      "\n",
      "FOLD-2==========\n",
      "val accuracy: 0.8098 \n",
      "\n",
      "FOLD-3==========\n",
      "val accuracy: 0.7975 \n",
      "\n",
      "FOLD-4==========\n",
      "val accuracy: 0.7791 \n",
      "\n",
      "FOLD-5==========\n",
      "val accuracy: 0.7853 \n",
      "\n",
      "FOLD-6==========\n",
      "val accuracy: 0.8200 \n",
      "\n",
      "FOLD-7==========\n",
      "val accuracy: 0.7832 \n",
      "\n",
      "FOLD-8==========\n",
      "val accuracy: 0.8384 \n",
      "\n",
      "FOLD-9==========\n",
      "val accuracy: 0.8221 \n",
      "\n",
      "FOLD-10==========\n",
      "val accuracy: 0.7812 \n",
      "\n",
      "K-Fold CV Accuracy: 0.8018\n",
      "Time taken in learning Decision Tree using entropy: 2.131 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree('entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4994ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "\n",
      "FOLD-1==========\n",
      "val accuracy: 0.8241 \n",
      "\n",
      "FOLD-2==========\n",
      "val accuracy: 0.7710 \n",
      "\n",
      "FOLD-3==========\n",
      "val accuracy: 0.8528 \n",
      "\n",
      "FOLD-4==========\n",
      "val accuracy: 0.7955 \n",
      "\n",
      "FOLD-5==========\n",
      "val accuracy: 0.8037 \n",
      "\n",
      "FOLD-6==========\n",
      "val accuracy: 0.8098 \n",
      "\n",
      "FOLD-7==========\n",
      "val accuracy: 0.7935 \n",
      "\n",
      "FOLD-8==========\n",
      "val accuracy: 0.7996 \n",
      "\n",
      "FOLD-9==========\n",
      "val accuracy: 0.7996 \n",
      "\n",
      "FOLD-10==========\n",
      "val accuracy: 0.8037 \n",
      "\n",
      "K-Fold CV Accuracy: 0.8053\n",
      "Time taken in learning Decision Tree using gini: 2.143 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree('gini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e632eb5",
   "metadata": {},
   "source": [
    "# Accuracy Report\n",
    "### 1. Accuracy for initial implementation (using entropy as impurity)\n",
    "The averaged accuracy reports on validation set for the initial implementation using entropy as impurity as an impurity measure came out to be **80.18%**. This is a decennt accuracy for an initial implementation just taking into consideration random shuffling of data, binary splits, entropy and information gain.\n",
    "### 2. Accuracy of improved implementation\n",
    "#### 2a. Gini-Index\n",
    "There is not much improvement in the averaged accuracy by using gini-index as impurity measure. The reported accuracy on validation set is **80.53** Minute difference in the accuracy can be seen. It may be possible that significant difference starts coming into the picute once the data increases in size and then time would also show certain difference. The way gain is computed, it takes less time in computig gini index as it does not take into consideration the impurity of parent node.\n",
    "#### 2b. Post Pruning\n",
    "The implementation for this was in between but could not be done on time. Here are some general thoughts on how this could improve the performance.\n",
    "1. Post pruning is a famous technique for avoiding overfitting in decision trees and hence the model will be able to perform better in terms of generalization error.\n",
    "2. The process goes like: it starts with finding out what are leaf nodes and decision nodes that has been misclassified by the algorithm. The idea is to prune the tree by removing such misclassifications from the tree and replacing them with the mostly occuring lead node value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46217bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
